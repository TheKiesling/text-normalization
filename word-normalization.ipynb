{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04d6f778",
   "metadata": {},
   "source": [
    "# Word Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29053ab",
   "metadata": {},
   "source": [
    "### José Pablo Kiesling Lange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27ec017c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "from tokenizers import Tokenizer, models, trainers, pre_tokenizers\n",
    "\n",
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf80513c",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_path = \"data/escher_comments.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6e16c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(corpus_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    corpus = file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4e84622",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [line.strip() for line in corpus if line.strip()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae86c91",
   "metadata": {},
   "source": [
    "## Estandarización"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7dd89a",
   "metadata": {},
   "source": [
    "Para ver la efectivdad de la estandarización, se mostrará las 10 palabras más frecuentes del corpus antes y después de la estandarización. El objetivo es ver si hay modificación en la cantidad de dichas palabras o si una nueva palabra aparece con frecuencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de3c16d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_common_words(corpus, n=10):\n",
    "    words = [word for line in corpus for word in line.split()]\n",
    "    most_common = Counter(words).most_common(n)\n",
    "    return most_common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44f920a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('que', 45),\n",
       " ('de', 39),\n",
       " ('la', 32),\n",
       " ('un', 27),\n",
       " ('una', 26),\n",
       " ('el', 17),\n",
       " ('en', 16),\n",
       " ('esfera', 15),\n",
       " ('reflejo', 15),\n",
       " ('y', 14)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_common_words = get_most_common_words(corpus, n=10)\n",
    "most_common_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9540c7b7",
   "metadata": {},
   "source": [
    "#### Case Folding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a0e6f8",
   "metadata": {},
   "source": [
    "Se hace una función que dado un corpus revisa si posee la misma palabra en mayúscula y minúscula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97bbce70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_upper_lower_words(corpus):\n",
    "    words = set(word for line in corpus for word in line.split())\n",
    "    upper_lower_words = set()\n",
    "    for word in words:\n",
    "        if word.lower() in words and word.upper() in words:\n",
    "            upper_lower_words.add(word)\n",
    "    return upper_lower_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47e89453",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A', 'a'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upper_lower_words = get_upper_lower_words(corpus)\n",
    "upper_lower_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69df04e",
   "metadata": {},
   "source": [
    "Dado que sí hay por lo menos una palabra que aparece en ambas formas, se procede a hacer un case folding (pasar todas las palabras a minúscula)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18cce89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def case_folding(corpus):\n",
    "    return [line.lower() for line in corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa0588d",
   "metadata": {},
   "source": [
    "#### Remove Punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8259178",
   "metadata": {},
   "source": [
    "Dado que en las instrucciones para generar el corpus se pidió mínimo 2 oraciones, se asume que mínimo una de las oraciones tiene puntuación. Por lo tanto, se procede a eliminar la puntuación del corpus.\n",
    "\n",
    "Esto se hace para evitar que palabras como `la,` o `la.` se cuenten como palabras diferentes a `la`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84702dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(corpus):\n",
    "    return [re.sub(r'[^\\w\\s]', '', line) for line in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77dcac30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standarizate(corpus):\n",
    "    corpus = case_folding(corpus)\n",
    "    corpus = remove_punctuation(corpus)\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff629776",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = standarizate(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc1db96",
   "metadata": {},
   "source": [
    "### Corpus ya estandarizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e5800cc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('que', 45),\n",
       " ('de', 39),\n",
       " ('la', 34),\n",
       " ('un', 30),\n",
       " ('una', 28),\n",
       " ('esfera', 18),\n",
       " ('en', 18),\n",
       " ('el', 17),\n",
       " ('reflejo', 16),\n",
       " ('se', 15)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_common_words = get_most_common_words(corpus, n=10)\n",
    "most_common_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4786f53",
   "metadata": {},
   "source": [
    "Como se puede apreciar:\n",
    "- la palabra `la` se incrementó en `2`ocurrencias\n",
    "- la palabra `un` se incrementó en `3` ocurrencias\n",
    "- la palabra `una` se incrementó en `3`ocurrencias\n",
    "- la palabra `esfera` se incrementó en `3` ocurrencias y subió al `6to` lugar\n",
    "- la palabra `en` se incrementó en `2` ocurrencias y subió al `7mo` lugar\n",
    "- la palabra `reflejo` se incrementó en `1` ocurrencia\n",
    "- la palabra `se` **ingresó** al top 10\n",
    "\n",
    "Por lo tanto, la estandarización ha tenido un efecto positivo en la, ya que ha incrementado la cantidad de ocurrencias de palabras comunes y ha eliminado las variaciones causadas por puntuación."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfa525a",
   "metadata": {},
   "source": [
    "Algo a recalcar es la ausencia de la estandarización de palabras con o sin tilde. Esto es porque se entrenará el modelo `BPE` en español."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298e3c75",
   "metadata": {},
   "source": [
    "## BPE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8a6b08",
   "metadata": {},
   "source": [
    "La implementación se hará en base a la implementación de `LangformersBlog`\n",
    "\n",
    "https://blog.langformers.com/bpe-tokenizer-explained/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad22c926",
   "metadata": {},
   "source": [
    "En la instanciación del `BpeTrainer` se especifica el tamaño del vocabulario y los tokens especiales. Para definir el tamaño del vocabulario, se debe considerar el tamaño del corpus y la cantidad de palabras que se espera que aparezcan en el corpus. Por lo tanto, se hacen métodos para calcular el tamaño del vocabulario y la cantidad de palabras que se espera que aparezcan en el corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "be419549",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab_size(corpus):\n",
    "    return len(set(word for line in corpus for word in line.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a860d0df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulario estimado por tokens únicos: 229\n"
     ]
    }
   ],
   "source": [
    "vocab_size = get_vocab_size(corpus)\n",
    "print(f\"Vocabulario estimado por tokens únicos: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe87f443",
   "metadata": {},
   "source": [
    "Dado que el corpus es pequeño, se define un tamaño de vocabulario de `300` y los tokens especiales son `[PAD]` y `[UNK]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "952f6735",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(models.BPE())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "77714657",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = trainers.BpeTrainer(\n",
    "    vocab_size=300,\n",
    "    special_tokens=[\"[PAD]\", \"[UNK]\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d98402",
   "metadata": {},
   "source": [
    "El token `[PAD]` se utiliza para rellenar secuencias de longitud variable, mientras que el token `[UNK]` se utiliza para representar palabras desconocidas o fuera del vocabulario.\n",
    "\n",
    "Ejemplo:\n",
    "Si el corpus contiene las palabras `hola`, `mundo`, y `adiós`, el vocabulario podría ser:\n",
    "\n",
    "```\n",
    "{\n",
    "    \"[PAD]\": 0,\n",
    "    \"[UNK]\": 1,\n",
    "    \"hola\": 2,\n",
    "    \"mundo\": 3,\n",
    "    \"adiós\": 4\n",
    "}\n",
    "```\n",
    "\n",
    "Entonces si viniera el texto `hello mundo adiós` y otro texto `hola mundo`, el tokenizador lo convertiría a\n",
    "1. [1, 3, 4] (donde `1` es el token `[UNK]` para `hello`)\n",
    "2. [2, 3, 0] (donde `0` es el token `[PAD]` para la secuencia de longitud variable)\n",
    "\n",
    "Es por ello que el uso de ambos tokens es importante para manejar secuencias de longitud variable y palabras desconocidas. En el caso de `PAD` es útil para rellenar secuencias más cortas, mientras que `UNK` es esencial para manejar palabras que no están en el vocabulario.\n",
    "\n",
    "Sin embargo, en el caso de `PAD`, normalmente no aparece en el vocabulario final porque se añade a las secuencias al momento de batching no durante el entrenamiento del tokenizador.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "69c3fa20",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.train_from_iterator(corpus, trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8443edf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: {'ebles ': 217, 'o de ': 72, 'en se ve ': 244, 'lo que ': 133, 'co': 106, 'una esfera reflectante ': 210, 'del ': 114, 'estra ': 266, 'perspec': 252, 'se ': 74, 'ando ': 161, 'á': 27, 'man': 138, 'alrede': 198, 'una esfera ': 86, 'va ': 96, 'entorno ': 271, 'ó': 31, 're': 40, 'ación de ': 258, 'or': 62, 'ver': 130, 'ación ': 137, 'un hombre ': 92, 'cen': 280, 'sostiene una esfera ': 256, 'esent': 159, 'reflejo ': 103, 'sí mismo ': 173, 'ce': 277, 'n': 15, 'pec': 148, 'el': 287, 'observ': 168, 'pción ': 294, 'es': 37, 'bita': 134, 'un ': 52, 'arec': 175, 'bros ': 197, 'ilu': 288, 'u': 22, 'veo ': 120, 'c': 5, 'or ': 132, 'li': 155, 'ient': 178, 'parece ': 208, 'les ': 180, 'sación de ': 298, 'y ': 71, 'observar ': 262, 'qu': 41, 'estr': 188, 'au': 123, 'iz': 219, 'endo ': 117, 'espe': 236, 'lo ': 84, 'so': 140, 'ob': 88, 'a': 3, 'perso': 251, 'im': 107, 'propi': 261, 'dos': 283, 'mente ': 207, 'más ': 139, 'tal ': 246, 'dis': 284, 'él ': 233, 'p': 17, 'present': 166, 'á ': 131, 'la ': 47, 'í': 29, 'sí': 128, 'sí m': 151, 'o ': 35, 'x': 24, 'su': 68, 'muebles ': 254, 'aliz': 249, 's': 20, 'entr': 240, 'una habitación ': 242, 'entor': 241, 'representación ': 267, 'mano ': 171, 'i': 11, 'iendo ': 126, 'su ': 93, 'mo ': 127, 'lu': 156, 'co ': 279, 'ent': 50, 'q': 18, 'in': 100, 'a ': 34, 'dad': 285, 'ment': 183, 'imient': 202, 'le': 46, 'm': 14, 'el reflejo de ': 195, 'en': 36, 'to': 141, 'ti': 65, 'habita': 135, 'todo ': 205, 'autor ': 263, 'se v': 110, 'un hombre que ': 147, 'pl': 293, 'ur': 129, 'perspectiva ': 272, 'ón ': 60, 'ene ': 102, 'alreded': 209, 'b': 4, 'cu': 176, 'est': 97, 'sen': 296, 'su reflejo ': 200, 'ser': 91, 'se ve ': 121, 'se': 228, 'de cr': 238, 'no ': 108, 'ún': 234, 'ión ': 179, 'pe': 94, 'spec': 229, 'reflect': 192, 'l': 13, 'os': 45, 'istal ': 250, 'dentr': 286, 'e ': 33, 'ombre ': 81, 'imag': 201, 'otr': 224, 'tur': 230, 'que mu': 237, 'veo a ': 257, 'tr': 75, 'ar': 105, 'esfera ': 73, 't': 21, 'em': 216, 'on': 223, 'do ': 61, 'pr': 79, 'v': 23, 'ct': 124, 'refleja ': 162, 'as': 274, 'tom': 299, 'tiene ': 104, 've': 95, 's ': 90, 'j': 12, 'de cristal ': 270, 'esfer': 69, 'o': 16, 'tiva ': 164, 'ten': 115, 'bi': 87, 'sent': 158, 'ñ': 30, 'cr': 213, 'e': 7, 'bu': 276, 'aut': 169, 'libros ': 206, 'jet': 221, 'an': 55, ' m': 122, 'en un ': 243, 'ag': 153, 'ombr': 78, 'que muestra ': 269, 'y': 25, 'ser ': 253, 'una esfera reflect': 199, 'la esfera ': 239, 'ú': 32, 'bo': 275, 'en la ': 163, ' ': 2, 'pu': 157, 'habitación ': 152, 'mbi': 222, 'rede': 189, 'for': 218, 'al ': 113, 'de la ': 143, 'is': 83, 'os ': 66, 'si': 85, 'pintur': 265, 'mu': 101, 'reflej': 56, 'el ': 54, 'jo ': 220, 'str': 297, 'ma ': 182, 'que es ': 191, 'ci': 59, 'reflexión ': 193, 'br': 64, 'hombre ': 82, 'fer': 67, 'z': 26, 'de': 98, 'me ': 181, 'sentimient': 260, 'per': 89, 'está ': 149, 'sosteniendo una esfera ': 264, 'ta': 70, 'una mano ': 194, 'er': 39, 'como ': 170, 'refle': 49, 'sosten': 145, 'd': 6, '[PAD]': 0, 'forma ': 268, 'ces ': 281, 'na ': 51, 'sos': 80, 'que ': 42, 'pi': 226, 'a la ': 109, 'nos': 291, 'ha': 99, 'sostiene ': 119, 'mos ': 184, 'parec': 186, 'con': 255, 'di': 154, 'observa ': 203, 'lo': 289, 'pro': 165, 'l ': 44, 'ción ': 76, 'xión ': 187, 'cur': 282, 'en el ': 196, 'es un ': 245, 'al': 77, 'una ': 53, 'n ': 38, 'et': 177, 'ás ': 116, 'ca ': 214, 'é': 28, 'ta ': 142, ' lo que ': 273, 'de ': 43, 'r ': 295, 'ar ': 167, 'pue': 259, 'ado ': 211, 'én ': 232, 'da la ': 215, 'ver ': 204, 'obser': 111, 'reflejo de ': 144, 'pin': 185, 'com': 125, 'los ': 290, 'f': 8, 'fle': 48, 'tra': 247, 'g': 9, 'có': 278, 'rec': 160, 'ant': 118, 'at': 174, 'h': 10, 'ad': 112, 'ismo ': 146, 'con ': 150, 'oca ': 225, 'alg': 248, 'po': 227, 'bles ': 212, 'es ': 63, 'sosteniendo ': 172, 'represent': 190, '[UNK]': 1, 'a que ': 235, 'en ': 58, 'r': 19, 'ante ': 136, 'voca ': 231, 'ndos': 292, 'om': 57}\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocabulary:\", tokenizer.get_vocab())\n",
    "tokenizer.save(\"tiny_tokenizer.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a978df4",
   "metadata": {},
   "source": [
    "## Comparación con WordPiece y SentencePiece"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1de15d",
   "metadata": {},
   "source": [
    "### WordPiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "94e3318a",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordPiece_tokenizer = Tokenizer(models.WordPiece(unk_token=\"[UNK]\"))\n",
    "wordPiece_tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "wordPiece_trainer = trainers.WordPieceTrainer(vocab_size=300, special_tokens=[\"[PAD]\", \"[UNK]\"])\n",
    "wordPiece_tokenizer.train_from_iterator(corpus, wordPiece_trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff86ade",
   "metadata": {},
   "source": [
    "### SentencePiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0f9b82ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentencePiece_tokenizer = Tokenizer(models.BPE())\n",
    "sentencePiece_tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "sentencePiece_trainer = trainers.BpeTrainer(vocab_size=300, special_tokens=[\"[PAD]\", \"[UNK]\"])\n",
    "sentencePiece_tokenizer.train_from_iterator(corpus, sentencePiece_trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ff92094d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = corpus[0][:60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bc42e255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample text: veo a un hombre explorando su propia percepción al sostener \n",
      "BPE Tokenization: ['veo a ', 'un hombre ', 'e', 'x', 'pl', 'or', 'ando ', 'su ', 'propi', 'a ', 'per', 'ce', 'pción ', 'al ', 'sosten', 'er', ' ']\n",
      "WordPiece Tokenization: ['veo', 'a', 'un', 'hombre', 'e', '##x', '##p', '##lo', '##ra', '##n', '##do', 'su', 'propi', '##a', 'per', '##ce', '##pción', 'al', 'sosten', '##er']\n",
      "SentencePiece Tokenization: ['veo', 'a', 'un', 'hombre', 'e', 'x', 'p', 'lo', 'r', 'ando', 'su', 'propia', 'percepción', 'al', 'sosten', 'er']\n"
     ]
    }
   ],
   "source": [
    "print(\"Sample text:\", sample)\n",
    "print(\"BPE Tokenization:\", tokenizer.encode(sample).tokens)\n",
    "print(\"WordPiece Tokenization:\", wordPiece_tokenizer.encode(sample).tokens)\n",
    "print(\"SentencePiece Tokenization:\", sentencePiece_tokenizer.encode(sample).tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301d91a9",
   "metadata": {},
   "source": [
    "Como se puede apreciar, en `BPE` aprende tokens frecuentes como secuencias `('veo a ', 'un hombre ')`. A veces no separa en palabras individuales, sino en fragmentos largos o combinaciones si son frecuentes. Puede incluir espacios dentro de los tokens.\n",
    "\n",
    "Por otro lado, `WordPiece` tiene una peculiaridad y es el uso de `##`para indicar que la subpalabra es parte de una palabra más larga. Además, fragmenta más las palabras conservando la semántica.\n",
    "\n",
    "Finalmente, `SentencePiece` es similar a `BPE` (lo cual hace sentido ya que este modelo usa este algoritmo), pero no utiliza espacios como separadores. Tampoco usa `##`pero puede segmentar de forma más limpias las palabras, ya que no depende de espacios.\n",
    "\n",
    "Por lo que en conclusión, `BPE` y `SentencePiece` son más flexibles en la segmentación de palabras, mientras que `WordPiece` es más estricto y conservador, lo que puede ser útil para modelos que requieren una segmentación más precisa de las palabras.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa46826",
   "metadata": {},
   "source": [
    "## Snowball Streamer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "57985740",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"spanish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "24d7ea7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_stemmed = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "378da229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: veo a , Stemmed: veo a \n",
      "Original: un hombre , Stemmed: un hombre \n",
      "Original: e, Stemmed: e\n",
      "Original: x, Stemmed: x\n",
      "Original: pl, Stemmed: pl\n",
      "Original: or, Stemmed: or\n",
      "Original: ando , Stemmed: ando \n",
      "Original: su , Stemmed: su \n",
      "Original: propi, Stemmed: propi\n",
      "Original: a , Stemmed: a \n",
      "Original: per, Stemmed: per\n",
      "Original: ce, Stemmed: ce\n",
      "Original: pción , Stemmed: pcion \n",
      "Original: al , Stemmed: al \n",
      "Original: sosten, Stemmed: sost\n",
      "Original: er, Stemmed: er\n",
      "Original:  , Stemmed:  \n",
      "\n",
      "Sample stemmed: veo a  un hombre  e x pl or ando  su  propi a  per ce pcion  al  sost er\n"
     ]
    }
   ],
   "source": [
    "for token in tokenizer.encode(sample).tokens:\n",
    "    sample_stemmed += stemmer.stem(token) + \" \"\n",
    "    print(f\"Original: {token}, Stemmed: {stemmer.stem(token)}\")\n",
    "    \n",
    "print(\"\\nSample stemmed:\", sample_stemmed.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53351146",
   "metadata": {},
   "source": [
    "Al hacer el stemming, se puede apreciar que por ejemplo la subpalabra `sosten` se convierte en `sost`. Además, la subpalabra `pción`se le remueve la tilde y se convierte en `pcion`.\n",
    "\n",
    "Sin embargo, en los otros casos, dado el algoritmo de tokenización usado, muchos tokens son dos palabras o fragmentos de palabras. Por lo que al hacer el stemming en dichos tokens, se tratan aisladamente, perdiendo la relación semántica original. Asismismo, muchas subpalabras tienen \"raíces\" que no son comunes en español, como `sosten` que se convierte en `sost`, lo cual no es una raíz válida en español.\n",
    "\n",
    "Por lo que respecto a la pérdida de información, se puede decir que el stemming no es efectivo en este caso, ya que no se logra una normalización adecuada de las palabras. Esto se debe a que el algoritmo de tokenización utilizado fragmenta las palabras de tal manera que al hacer el stemming se pierde la relación semántica original."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

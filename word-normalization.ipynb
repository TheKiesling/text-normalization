{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04d6f778",
   "metadata": {},
   "source": [
    "# Word Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29053ab",
   "metadata": {},
   "source": [
    "### José Pablo Kiesling Lange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27ec017c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "from tokenizers import Tokenizer, models, trainers, pre_tokenizers\n",
    "\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "from Levenshtein import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf80513c",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_path = \"data/escher_comments.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6e16c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(corpus_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    corpus = file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4e84622",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [line.strip() for line in corpus if line.strip()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae86c91",
   "metadata": {},
   "source": [
    "## Estandarización"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7dd89a",
   "metadata": {},
   "source": [
    "Para ver la efectivdad de la estandarización, se mostrará las 10 palabras más frecuentes del corpus antes y después de la estandarización. El objetivo es ver si hay modificación en la cantidad de dichas palabras o si una nueva palabra aparece con frecuencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de3c16d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_common_words(corpus, n=10):\n",
    "    words = [word for line in corpus for word in line.split()]\n",
    "    most_common = Counter(words).most_common(n)\n",
    "    return most_common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44f920a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('que', 45),\n",
       " ('de', 39),\n",
       " ('la', 32),\n",
       " ('un', 27),\n",
       " ('una', 26),\n",
       " ('el', 17),\n",
       " ('en', 16),\n",
       " ('esfera', 15),\n",
       " ('reflejo', 15),\n",
       " ('y', 14)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_common_words = get_most_common_words(corpus, n=10)\n",
    "most_common_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9540c7b7",
   "metadata": {},
   "source": [
    "#### Case Folding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a0e6f8",
   "metadata": {},
   "source": [
    "Se hace una función que dado un corpus revisa si posee la misma palabra en mayúscula y minúscula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97bbce70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_upper_lower_words(corpus):\n",
    "    words = set(word for line in corpus for word in line.split())\n",
    "    upper_lower_words = set()\n",
    "    for word in words:\n",
    "        if word.lower() in words and word.upper() in words:\n",
    "            upper_lower_words.add(word)\n",
    "    return upper_lower_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47e89453",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A', 'a'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upper_lower_words = get_upper_lower_words(corpus)\n",
    "upper_lower_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69df04e",
   "metadata": {},
   "source": [
    "Dado que sí hay por lo menos una palabra que aparece en ambas formas, se procede a hacer un case folding (pasar todas las palabras a minúscula)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18cce89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def case_folding(corpus):\n",
    "    return [line.lower() for line in corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa0588d",
   "metadata": {},
   "source": [
    "#### Remove Punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8259178",
   "metadata": {},
   "source": [
    "Dado que en las instrucciones para generar el corpus se pidió mínimo 2 oraciones, se asume que mínimo una de las oraciones tiene puntuación. Por lo tanto, se procede a eliminar la puntuación del corpus.\n",
    "\n",
    "Esto se hace para evitar que palabras como `la,` o `la.` se cuenten como palabras diferentes a `la`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84702dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(corpus):\n",
    "    return [re.sub(r'[^\\w\\s]', '', line) for line in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77dcac30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standarizate(corpus):\n",
    "    corpus = case_folding(corpus)\n",
    "    corpus = remove_punctuation(corpus)\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff629776",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = standarizate(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc1db96",
   "metadata": {},
   "source": [
    "### Corpus ya estandarizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e5800cc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('que', 45),\n",
       " ('de', 39),\n",
       " ('la', 34),\n",
       " ('un', 30),\n",
       " ('una', 28),\n",
       " ('esfera', 18),\n",
       " ('en', 18),\n",
       " ('el', 17),\n",
       " ('reflejo', 16),\n",
       " ('se', 15)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_common_words = get_most_common_words(corpus, n=10)\n",
    "most_common_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4786f53",
   "metadata": {},
   "source": [
    "Como se puede apreciar:\n",
    "- la palabra `la` se incrementó en `2`ocurrencias\n",
    "- la palabra `un` se incrementó en `3` ocurrencias\n",
    "- la palabra `una` se incrementó en `3`ocurrencias\n",
    "- la palabra `esfera` se incrementó en `3` ocurrencias y subió al `6to` lugar\n",
    "- la palabra `en` se incrementó en `2` ocurrencias y subió al `7mo` lugar\n",
    "- la palabra `reflejo` se incrementó en `1` ocurrencia\n",
    "- la palabra `se` **ingresó** al top 10\n",
    "\n",
    "Por lo tanto, la estandarización ha tenido un efecto positivo en la, ya que ha incrementado la cantidad de ocurrencias de palabras comunes y ha eliminado las variaciones causadas por puntuación."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfa525a",
   "metadata": {},
   "source": [
    "Algo a recalcar es la ausencia de la estandarización de palabras con o sin tilde. Esto es porque se entrenará el modelo `BPE` en español."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298e3c75",
   "metadata": {},
   "source": [
    "## BPE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8a6b08",
   "metadata": {},
   "source": [
    "La implementación se hará en base a la implementación de `LangformersBlog`\n",
    "\n",
    "https://blog.langformers.com/bpe-tokenizer-explained/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad22c926",
   "metadata": {},
   "source": [
    "En la instanciación del `BpeTrainer` se especifica el tamaño del vocabulario y los tokens especiales. Para definir el tamaño del vocabulario, se debe considerar el tamaño del corpus y la cantidad de palabras que se espera que aparezcan en el corpus. Por lo tanto, se hacen métodos para calcular el tamaño del vocabulario y la cantidad de palabras que se espera que aparezcan en el corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "be419549",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab_size(corpus):\n",
    "    return len(set(word for line in corpus for word in line.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a860d0df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulario estimado por tokens únicos: 229\n"
     ]
    }
   ],
   "source": [
    "vocab_size = get_vocab_size(corpus)\n",
    "print(f\"Vocabulario estimado por tokens únicos: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe87f443",
   "metadata": {},
   "source": [
    "Dado que el corpus es pequeño, se define un tamaño de vocabulario de `300` y los tokens especiales son `[PAD]` y `[UNK]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "952f6735",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(models.BPE())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "77714657",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = trainers.BpeTrainer(\n",
    "    vocab_size=300,\n",
    "    special_tokens=[\"[PAD]\", \"[UNK]\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d98402",
   "metadata": {},
   "source": [
    "El token `[PAD]` se utiliza para rellenar secuencias de longitud variable, mientras que el token `[UNK]` se utiliza para representar palabras desconocidas o fuera del vocabulario.\n",
    "\n",
    "Ejemplo:\n",
    "Si el corpus contiene las palabras `hola`, `mundo`, y `adiós`, el vocabulario podría ser:\n",
    "\n",
    "```\n",
    "{\n",
    "    \"[PAD]\": 0,\n",
    "    \"[UNK]\": 1,\n",
    "    \"hola\": 2,\n",
    "    \"mundo\": 3,\n",
    "    \"adiós\": 4\n",
    "}\n",
    "```\n",
    "\n",
    "Entonces si viniera el texto `hello mundo adiós` y otro texto `hola mundo`, el tokenizador lo convertiría a\n",
    "1. [1, 3, 4] (donde `1` es el token `[UNK]` para `hello`)\n",
    "2. [2, 3, 0] (donde `0` es el token `[PAD]` para la secuencia de longitud variable)\n",
    "\n",
    "Es por ello que el uso de ambos tokens es importante para manejar secuencias de longitud variable y palabras desconocidas. En el caso de `PAD` es útil para rellenar secuencias más cortas, mientras que `UNK` es esencial para manejar palabras que no están en el vocabulario.\n",
    "\n",
    "Sin embargo, en el caso de `PAD`, normalmente no aparece en el vocabulario final porque se añade a las secuencias al momento de batching no durante el entrenamiento del tokenizador.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "69c3fa20",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.train_from_iterator(corpus, trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8443edf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: {'ombre ': 81, 'sí mismo ': 173, 'v': 23, 'ti': 65, 'bros ': 197, 'que es ': 191, 'tiene ': 104, 'so': 140, 'mano ': 171, 'ver ': 204, 'jo ': 220, 'u': 22, 'li': 155, 'qu': 41, 'ta': 70, 'pi': 226, 'de la ': 143, 'or ': 132, 'pu': 157, 'sen': 296, 'una mano ': 194, 'con': 255, 'do ': 61, 'ebles ': 217, 'de': 98, 'oca ': 225, 'bu': 276, 'da la ': 215, 'ar ': 167, 'espe': 236, 'pl': 293, 'sí m': 151, 'alrede': 198, 'reflexión ': 193, '[UNK]': 1, 'co ': 279, 'es un ': 245, 'no ': 108, 'refle': 49, 'x': 24, 'parec': 186, 'a': 3, 'alreded': 209, 'ad': 112, 'mos ': 184, 't': 21, 'en un ': 243, 'ene ': 102, 's ': 90, 'on': 223, 'alg': 248, 'entorno ': 271, 'lu': 156, 'más ': 139, 'r': 19, 'arec': 175, 'se v': 110, 'ando ': 161, 'bles ': 212, ' lo que ': 273, 'b': 4, 'libros ': 206, 'bita': 134, 'n': 15, 'po': 227, 'en ': 58, 'cu': 176, 'ó': 31, 'y': 25, 'refleja ': 162, 'e ': 33, 'propi': 261, 'fer': 67, 'co': 106, 'en': 36, 'pro': 165, 'mente ': 207, 'su ': 93, 'está ': 149, 'p': 17, 'e': 7, 'bi': 87, 'for': 218, 'en se ve ': 244, 'les ': 180, 'una esfera ': 86, 'as': 274, 'representación ': 267, 'f': 8, 'habita': 135, 'ión ': 179, 'represent': 190, 'en el ': 196, 'sos': 80, 'om': 57, 're': 40, 'un ': 52, 'al': 77, 'esfera ': 73, 'como ': 170, 'una ': 53, 'em': 216, 'br': 64, 'ob': 88, 'otr': 224, ' ': 2, 'se ': 74, 'ación ': 137, 'el': 287, 'perspectiva ': 272, 'n ': 38, 'se': 228, 'ces ': 281, 'est': 97, 'iz': 219, 'á': 27, 'en la ': 163, 'pción ': 294, 'os': 45, 'reflej': 56, 'muebles ': 254, 'is': 83, 'o': 16, 'ú': 32, 'a la ': 109, 'm': 14, 'estr': 188, 'g': 9, 'aliz': 249, 'de cristal ': 270, 'un hombre que ': 147, 'au': 123, 'autor ': 263, 'veo a ': 257, 'man': 138, 'sosteniendo ': 172, 'ten': 115, 'si': 85, 'sí': 128, 'los ': 290, 'fle': 48, 'su reflejo ': 200, 'todo ': 205, 'ado ': 211, 'd': 6, 'sación de ': 298, 'í': 29, 'pec': 148, 'imag': 201, 'con ': 150, 'o ': 35, 'ilu': 288, 'observa ': 203, 'a ': 34, 'ás ': 116, 'present': 166, 'bo': 275, 'reflect': 192, 'á ': 131, 'ar': 105, 'pue': 259, 'pe': 94, 'se ve ': 121, 'que mu': 237, 'jet': 221, 'mo ': 127, 'endo ': 117, 'esfer': 69, 'cr': 213, 'entor': 241, 'él ': 233, 'tr': 75, 'una habitación ': 242, 'spec': 229, 'h': 10, 'ha': 99, 'q': 18, 'la ': 47, 'me ': 181, 'de ': 43, 'veo ': 120, 'có': 278, 'os ': 66, 'én ': 232, 'pin': 185, 'que muestra ': 269, 'ante ': 136, 'per': 89, 'nos': 291, 'na ': 51, 'ombr': 78, 'ñ': 30, 'sostiene una esfera ': 256, 'ant': 118, 'la esfera ': 239, 'ta ': 142, 'pr': 79, 'observar ': 262, 'i': 11, 'l': 13, 'ce': 277, 'ción ': 76, 'sent': 158, 'estra ': 266, 'ún': 234, 'entr': 240, 'va ': 96, 'ag': 153, 'dentr': 286, 'tal ': 246, 'com': 125, 'perso': 251, 'obser': 111, 'ma ': 182, 'cen': 280, 'an': 55, 'et': 177, 'ci': 59, 'im': 107, 'ver': 130, 'ct': 124, 'su': 68, 'tra': 247, 'una esfera reflect': 199, 'ment': 183, 'aut': 169, 'pintur': 265, 'el ': 54, 'sosteniendo una esfera ': 264, 'or': 62, 'j': 12, 'str': 297, 'ient': 178, 'hombre ': 82, 'c': 5, 'tur': 230, 'le': 46, 'ca ': 214, 'cur': 282, 'que ': 42, 'al ': 113, 'ndos': 292, 'y ': 71, 'ón ': 60, 'ur': 129, 'at': 174, 'perspec': 252, 'una esfera reflectante ': 210, 'dis': 284, 'sosten': 145, 'xión ': 187, 'observ': 168, 'er': 39, 'dad': 285, 've': 95, 'ismo ': 146, 'tom': 299, 'to': 141, 'reflejo ': 103, 'lo': 289, 'istal ': 250, 'ación de ': 258, 'ser': 91, 'mbi': 222, 'es ': 63, 'sostiene ': 119, 'mu': 101, 'rede': 189, 'in': 100, ' m': 122, 'del ': 114, 'r ': 295, 'voca ': 231, 'a que ': 235, 'habitación ': 152, 'ent': 50, 'un hombre ': 92, 'iendo ': 126, 'forma ': 268, 'reflejo de ': 144, 'rec': 160, 'di': 154, 'el reflejo de ': 195, 'esent': 159, 'imient': 202, 'parece ': 208, 'lo ': 84, 'é': 28, 'lo que ': 133, 'l ': 44, 'z': 26, 'sentimient': 260, 'de cr': 238, 'es': 37, 'tiva ': 164, 'ser ': 253, '[PAD]': 0, 'dos': 283, 'o de ': 72, 's': 20}\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocabulary:\", tokenizer.get_vocab())\n",
    "tokenizer.save(\"tiny_tokenizer.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a978df4",
   "metadata": {},
   "source": [
    "## Comparación con WordPiece y SentencePiece"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1de15d",
   "metadata": {},
   "source": [
    "### WordPiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "94e3318a",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordPiece_tokenizer = Tokenizer(models.WordPiece(unk_token=\"[UNK]\"))\n",
    "wordPiece_tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "wordPiece_trainer = trainers.WordPieceTrainer(vocab_size=300, special_tokens=[\"[PAD]\", \"[UNK]\"])\n",
    "wordPiece_tokenizer.train_from_iterator(corpus, wordPiece_trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff86ade",
   "metadata": {},
   "source": [
    "### SentencePiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0f9b82ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentencePiece_tokenizer = Tokenizer(models.BPE())\n",
    "sentencePiece_tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "sentencePiece_trainer = trainers.BpeTrainer(vocab_size=300, special_tokens=[\"[PAD]\", \"[UNK]\"])\n",
    "sentencePiece_tokenizer.train_from_iterator(corpus, sentencePiece_trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ff92094d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = corpus[0][:60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bc42e255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample text: veo a un hombre explorando su propia percepción al sostener \n",
      "BPE Tokenization: ['veo a ', 'un hombre ', 'e', 'x', 'pl', 'or', 'ando ', 'su ', 'propi', 'a ', 'per', 'ce', 'pción ', 'al ', 'sosten', 'er', ' ']\n",
      "WordPiece Tokenization: ['veo', 'a', 'un', 'hombre', 'e', '##x', '##p', '##lo', '##ra', '##n', '##do', 'su', 'propi', '##a', 'per', '##ce', '##pción', 'al', 'sosten', '##er']\n",
      "SentencePiece Tokenization: ['veo', 'a', 'un', 'hombre', 'e', 'x', 'p', 'lo', 'r', 'ando', 'su', 'propia', 'percepción', 'al', 'sosten', 'er']\n"
     ]
    }
   ],
   "source": [
    "print(\"Sample text:\", sample)\n",
    "print(\"BPE Tokenization:\", tokenizer.encode(sample).tokens)\n",
    "print(\"WordPiece Tokenization:\", wordPiece_tokenizer.encode(sample).tokens)\n",
    "print(\"SentencePiece Tokenization:\", sentencePiece_tokenizer.encode(sample).tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301d91a9",
   "metadata": {},
   "source": [
    "Como se puede apreciar, en `BPE` aprende tokens frecuentes como secuencias `('veo a ', 'un hombre ')`. A veces no separa en palabras individuales, sino en fragmentos largos o combinaciones si son frecuentes. Puede incluir espacios dentro de los tokens.\n",
    "\n",
    "Por otro lado, `WordPiece` tiene una peculiaridad y es el uso de `##`para indicar que la subpalabra es parte de una palabra más larga. Además, fragmenta más las palabras conservando la semántica.\n",
    "\n",
    "Finalmente, `SentencePiece` es similar a `BPE` (lo cual hace sentido ya que este modelo usa este algoritmo), pero no utiliza espacios como separadores. Tampoco usa `##`pero puede segmentar de forma más limpias las palabras, ya que no depende de espacios.\n",
    "\n",
    "Por lo que en conclusión, `BPE` y `SentencePiece` son más flexibles en la segmentación de palabras, mientras que `WordPiece` es más estricto y conservador, lo que puede ser útil para modelos que requieren una segmentación más precisa de las palabras.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa46826",
   "metadata": {},
   "source": [
    "## Snowball Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "57985740",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"spanish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "24d7ea7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_stemmed = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ca618c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = sample.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "378da229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: veo, Stemmed: veo\n",
      "Original: a, Stemmed: a\n",
      "Original: un, Stemmed: un\n",
      "Original: hombre, Stemmed: hombr\n",
      "Original: explorando, Stemmed: explor\n",
      "Original: su, Stemmed: su\n",
      "Original: propia, Stemmed: propi\n",
      "Original: percepción, Stemmed: percepcion\n",
      "Original: al, Stemmed: al\n",
      "Original: sostener, Stemmed: sosten\n",
      "\n",
      "Sample stemmed: veo a un hombr explor su propi percepcion al sosten\n"
     ]
    }
   ],
   "source": [
    "for token in words:\n",
    "    sample_stemmed += stemmer.stem(token) + \" \"\n",
    "    print(f\"Original: {token}, Stemmed: {stemmer.stem(token)}\")\n",
    "    \n",
    "print(\"\\nSample stemmed:\", sample_stemmed.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53351146",
   "metadata": {},
   "source": [
    "Al hacer el stemming, se puede apreciar que verbos mantienen su raíz para que se pueda entender su lema. Por ejemplo, la subpalabra `explorando` se convierte en `explor`, la cual puede tener diferentes conjugaciones, y se sigue entendiendo como `explorar`.\n",
    "Sin embargo, en el caso de sustantivos, se pierde la relación semántica original. Por ejemplo, la subpalabra `hombr` se puede entender como `hombre` u `hombro`. Esto se debe a que el algoritmo de stemming utilizado no tiene en cuenta el contexto de la palabra, por lo que puede generar raíces ambiguas.\n",
    "\n",
    "Además, la subpalabra `percepción`se le remueve la tilde y se convierte en `percepcion`. Lo cual puede ser problemático, ya que en español la tilde es importante para la correcta pronunciación y significado de las palabras. Por lo tanto, al hacer el stemming, se pierde información importante sobre la palabra.\n",
    "\n",
    "Por lo tanto, el stemming es útil para reducir las palabras a su raíz, pero puede generar raíces ambiguas y perder información importante sobre la palabra. En este caso, se recomienda utilizar lematización en lugar de stemming para preservar la relación semántica original de las palabras."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59794e3c",
   "metadata": {},
   "source": [
    "## Levenshtein"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
